# scripts/analyze_alignment.py

"""
This script analyzes the alignment between pre-computed visual and text
features loaded via a manifest.

It computes retrieval metrics (Recall@K) and similarity statistics
based on a "ground truth" (gt) mask.

The mask is generated by parsing a grouping column (e.g., 'volume_name')
to extract a shared exam ID (e.g., 'train_1_a_1' -> 'train_1_a').
"""

import logging
import os
import sys

import hydra
import torch
import numpy as np 
import pandas as pd
from omegaconf import DictConfig, OmegaConf
from torch.utils.data import DataLoader
from tqdm.auto import tqdm

# Add the project root to the path to allow importing from 'common'
# This is necessary because this script is in a subdirectory.
sys.path.append(os.path.abspath(os.path.join(__file__, "../..")))

from common.data.dataset import FeatureDataset  # noqa: E402
from common.utils import set_seed  # noqa: E402

# Configure a logger for this script
log = logging.getLogger(__name__)

# Define the K values for recall@k as a single source of truth
TOP_K_VALUES = [1, 5, 10, 50]

# Define K values for NDCG metric
NDCG_K_VALUES = [5, 10, 50]

@torch.no_grad()
def calculate_metrics(
    similarity_matrix: torch.Tensor,
    gt_mask: torch.Tensor,
    *,
    prefix: str = "",
    include_sample_count: bool = True,
) -> dict:
    """
    Calculates retrieval metrics from a similarity matrix using a
    pre-computed ground truth (gt) mask.

    Args:
        similarity_matrix: A (N, N) tensor where S[i, j] is the
            cosine similarity between visual feature i and text feature j.
        gt_mask: A (N, N) boolean tensor where gt_mask[i, j] is True
            if (visual_i, text_j) is a "correct" pair.

    Returns:
        A dictionary containing computed metrics.
    """
    num_samples = similarity_matrix.shape[0]
    if num_samples == 0:
        return {}

    if gt_mask.shape != similarity_matrix.shape:
        raise ValueError(
            f"gt_mask shape ({gt_mask.shape}) must match "
            f"similarity_matrix shape ({similarity_matrix.shape})"
        )

    # --- Mean Similarities (Based on GT Mask) ---

    # We must differentiate between instance-level (prefix == "") and
    # semantic-level (prefix != "") similarity calculation.
    # Instance-level compares N-to-N exact pairs (the diagonal).
    # Semantic-level compares N-to-N semantic matches (the gt_mask).
    if prefix:
        # Semantic-level: Use the gt_mask
        paired_values = similarity_matrix[gt_mask]
        if paired_values.numel():
            mean_paired_similarity = paired_values.mean().item()
        else:
            mean_paired_similarity = float("nan")

        unpaired_values = similarity_matrix[~gt_mask]
        if unpaired_values.numel():
            mean_unpaired_similarity = unpaired_values.mean().item()
        else:
            mean_unpaired_similarity = float("nan")
    else:
        # Instance-level: Use the diagonal (N exact pairs)
        paired_values = similarity_matrix.diag()
        mean_paired_similarity = paired_values.mean().item()

        # Use all non-diagonal elements for unpaired (N*N - N elements)
        unpaired_mask = torch.ones_like(
            similarity_matrix, dtype=torch.bool
        ).fill_diagonal_(False)
        
        unpaired_values = similarity_matrix[unpaired_mask]
        if unpaired_values.numel():
            mean_unpaired_similarity = unpaired_values.mean().item()
        else:
            mean_unpaired_similarity = float("nan")

    # --- Retrieval Metrics (Recall@K) ---
    metrics = {}

    if include_sample_count:
        count_key = f"{prefix}samples_analyzed" if prefix else "samples_analyzed"
        metrics[count_key] = num_samples

    metrics[f"{prefix}mean_paired_similarity"] = mean_paired_similarity
    metrics[f"{prefix}mean_unpaired_similarity"] = mean_unpaired_similarity

    max_k = min(max(TOP_K_VALUES), num_samples)

    # 3. Visual-to-Text (V->T) Recall@K
    v_to_t_top_k_indices = torch.topk(
        similarity_matrix, k=max_k, dim=1
    ).indices
    v_to_t_hits = gt_mask.gather(dim=1, index=v_to_t_top_k_indices)

    # 4. Text-to-Visual (T->V) Recall@K
    t_to_v_top_k_indices = torch.topk(
        similarity_matrix, k=max_k, dim=0
    ).indices
    t_to_v_hits = gt_mask.T.gather(dim=1, index=t_to_v_top_k_indices.T)

    # Calculate Recall@K for each k
    for k in TOP_K_VALUES:
        if k > max_k:
            continue
            
        v_to_t_recall_at_k = v_to_t_hits[:, :k].any(dim=1).float().mean().item()
        metrics[f"{prefix}v_to_t_recall_at_{k}"] = v_to_t_recall_at_k

        t_to_v_recall_at_k = t_to_v_hits[:, :k].any(dim=1).float().mean().item()
        metrics[f"{prefix}t_to_v_recall_at_{k}"] = t_to_v_recall_at_k

    # --- Mean Reciprocal Rank (MRR) ---
    # V->T MRR: For each visual query, find rank of first correct text match
    v_to_t_ranks = torch.where(v_to_t_hits)[1]  # Get column indices of hits
    v_to_t_first_ranks = torch.full((num_samples,), max_k + 1, dtype=torch.long, device=similarity_matrix.device)
    
    # For each sample, find the minimum rank (first occurrence)
    for i in tqdm(range(num_samples), desc="Computing V->T MRR", leave=False):
        sample_hits = torch.where(v_to_t_hits[i])[0]
        if sample_hits.numel() > 0:
            v_to_t_first_ranks[i] = sample_hits[0]
    
    # Calculate reciprocal rank (add 1 since ranks are 0-indexed)
    v_to_t_reciprocal_ranks = 1.0 / (v_to_t_first_ranks.float() + 1.0)
    v_to_t_reciprocal_ranks[v_to_t_first_ranks > max_k] = 0.0  # No hit found
    v_to_t_mrr = v_to_t_reciprocal_ranks.mean().item()
    metrics[f"{prefix}v_to_t_mrr"] = v_to_t_mrr

    # T->V MRR: For each text query, find rank of first correct visual match
    t_to_v_ranks = torch.where(t_to_v_hits)[1]
    t_to_v_first_ranks = torch.full((num_samples,), max_k + 1, dtype=torch.long, device=similarity_matrix.device)
    
    for i in tqdm(range(num_samples), desc="Computing T->V MRR", leave=False):
        sample_hits = torch.where(t_to_v_hits[i])[0]
        if sample_hits.numel() > 0:
            t_to_v_first_ranks[i] = sample_hits[0]
    
    t_to_v_reciprocal_ranks = 1.0 / (t_to_v_first_ranks.float() + 1.0)
    t_to_v_reciprocal_ranks[t_to_v_first_ranks > max_k] = 0.0
    t_to_v_mrr = t_to_v_reciprocal_ranks.mean().item()
    metrics[f"{prefix}t_to_v_mrr"] = t_to_v_mrr

    # --- Mean Average Precision (MAP) ---
    # V->T MAP: Average precision for each visual query
    v_to_t_ap_list = []
    for i in tqdm(range(num_samples), desc="Computing V->T MAP", leave=False):
        hits_at_i = v_to_t_hits[i].float()
        num_relevant = hits_at_i.sum().item()
        
        if num_relevant == 0:
            v_to_t_ap_list.append(0.0)
            continue
        
        # Calculate precision at each position where there's a hit
        cumsum_hits = torch.cumsum(hits_at_i, dim=0)
        ranks = torch.arange(1, max_k + 1, device=similarity_matrix.device, dtype=torch.float32)
        precisions_at_k = cumsum_hits / ranks
        
        # Average precision: mean of precisions at relevant positions
        ap = (precisions_at_k * hits_at_i).sum().item() / num_relevant
        v_to_t_ap_list.append(ap)
    
    v_to_t_map = sum(v_to_t_ap_list) / len(v_to_t_ap_list) if v_to_t_ap_list else 0.0
    metrics[f"{prefix}v_to_t_map"] = v_to_t_map

    # T->V MAP: Average precision for each text query
    t_to_v_ap_list = []
    for i in tqdm(range(num_samples), desc="Computing T->V MAP", leave=False):
        hits_at_i = t_to_v_hits[i].float()
        num_relevant = hits_at_i.sum().item()
        
        if num_relevant == 0:
            t_to_v_ap_list.append(0.0)
            continue
        
        cumsum_hits = torch.cumsum(hits_at_i, dim=0)
        ranks = torch.arange(1, max_k + 1, device=similarity_matrix.device, dtype=torch.float32)
        precisions_at_k = cumsum_hits / ranks
        
        ap = (precisions_at_k * hits_at_i).sum().item() / num_relevant
        t_to_v_ap_list.append(ap)
    
    t_to_v_map = sum(t_to_v_ap_list) / len(t_to_v_ap_list) if t_to_v_ap_list else 0.0
    metrics[f"{prefix}t_to_v_map"] = t_to_v_map

    # --- Normalized Discounted Cumulative Gain (NDCG@K) ---
    for k in NDCG_K_VALUES:
        if k > max_k:
            continue
        
        # V->T NDCG@K
        v_to_t_dcg_list = []
        v_to_t_idcg_list = []
        
        for i in tqdm(range(num_samples), desc=f"Computing V->T NDCG@{k}", leave=False):
            hits_at_i = v_to_t_hits[i, :k].float()
            num_relevant = v_to_t_hits[i].sum().item()
            
            # DCG: sum of rel / log2(rank + 1)
            ranks = torch.arange(1, k + 1, device=similarity_matrix.device, dtype=torch.float32)
            dcg = (hits_at_i / torch.log2(ranks + 1)).sum().item()
            v_to_t_dcg_list.append(dcg)
            
            # IDCG: ideal DCG (all relevant items at top)
            num_relevant_at_k = min(num_relevant, k)
            if num_relevant_at_k > 0:
                ideal_hits = torch.zeros(k, device=similarity_matrix.device)
                ideal_hits[:int(num_relevant_at_k)] = 1.0
                idcg = (ideal_hits / torch.log2(ranks + 1)).sum().item()
                v_to_t_idcg_list.append(idcg)
            else:
                v_to_t_idcg_list.append(0.0)
        
        # NDCG = DCG / IDCG, handle division by zero
        v_to_t_ndcg_values = [
            dcg / idcg if idcg > 0 else 0.0
            for dcg, idcg in zip(v_to_t_dcg_list, v_to_t_idcg_list)
        ]
        v_to_t_ndcg = sum(v_to_t_ndcg_values) / len(v_to_t_ndcg_values) if v_to_t_ndcg_values else 0.0
        metrics[f"{prefix}v_to_t_ndcg_at_{k}"] = v_to_t_ndcg
        
        # T->V NDCG@K
        t_to_v_dcg_list = []
        t_to_v_idcg_list = []
        
        for i in tqdm(range(num_samples), desc=f"Computing T->V NDCG@{k}", leave=False):
            hits_at_i = t_to_v_hits[i, :k].float()
            num_relevant = t_to_v_hits[i].sum().item()
            
            ranks = torch.arange(1, k + 1, device=similarity_matrix.device, dtype=torch.float32)
            dcg = (hits_at_i / torch.log2(ranks + 1)).sum().item()
            t_to_v_dcg_list.append(dcg)
            
            num_relevant_at_k = min(num_relevant, k)
            if num_relevant_at_k > 0:
                ideal_hits = torch.zeros(k, device=similarity_matrix.device)
                ideal_hits[:int(num_relevant_at_k)] = 1.0
                idcg = (ideal_hits / torch.log2(ranks + 1)).sum().item()
                t_to_v_idcg_list.append(idcg)
            else:
                t_to_v_idcg_list.append(0.0)
        
        t_to_v_ndcg_values = [
            dcg / idcg if idcg > 0 else 0.0
            for dcg, idcg in zip(t_to_v_dcg_list, t_to_v_idcg_list)
        ]
        t_to_v_ndcg = sum(t_to_v_ndcg_values) / len(t_to_v_ndcg_values) if t_to_v_ndcg_values else 0.0
        metrics[f"{prefix}t_to_v_ndcg_at_{k}"] = t_to_v_ndcg

    return metrics


def analyze_alignment(cfg: DictConfig) -> None:
    """
    Hydra-configured function to load features and run alignment analysis.
    """
    log.info("Starting alignment analysis...")
    try:
        cfg_repr = OmegaConf.to_yaml(cfg)
    except Exception:
        cfg_repr = str(cfg)
    log.info(f"Full configuration:\n{cfg_repr}")

    # --- 1. Validation ---
    visual_col = OmegaConf.select(cfg, "data.columns.visual_feature")
    text_col = OmegaConf.select(cfg, "data.columns.text_feature")
    label_cols = OmegaConf.select(cfg, "data.columns.labels", default=[])
    
    # Se establece 'volume_name' como el valor por defecto
    grouping_col = OmegaConf.select(
        cfg, "data.columns.grouping_col", default="volumename"
    )
    
    # Option to filter out normal cases (all labels zero)
    filter_normal_cases = OmegaConf.select(
        cfg, "analysis.filter_normal_cases", default=False
    )

    if not visual_col or not text_col:
        raise ValueError("Missing 'data.columns.visual_feature' or '...text_feature'.")
        
    # El bloque 'if not grouping_col:' se elimina completamente.

    if label_cols is None:
        label_cols = []
    elif OmegaConf.is_list(label_cols):
        label_cols = list(label_cols)
    elif isinstance(label_cols, (list, tuple)):
        label_cols = list(label_cols)
    elif label_cols:
        label_cols = [str(label_cols)]
    else:
        label_cols = []

    log.info(f"Using visual feature column: '{visual_col}'")
    log.info(f"Using text feature column:   '{text_col}'")
    log.info(f"Using grouping string column (to parse): '{grouping_col}'")
    log.info(f"Filter normal cases (all labels zero): {filter_normal_cases}")
    if label_cols:
        log.info(f"Semantic metrics enabled with {len(label_cols)} label columns.")
    else:
        log.info("Semantic metrics disabled (no label columns provided).")

    # --- 2. Setup ---
    set_seed(cfg.utils.seed)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    log.info(f"Using device: {device}")

    # --- 3. Load Data ---
    target_manifest = OmegaConf.select(cfg, "data.train_manifest")
    log.info(f"Target manifest file: {target_manifest}")

    manifest_path = os.path.normpath(
        os.path.join(cfg.paths.manifest_dir, target_manifest)
    )

    # Manually load manifest with pandas to get string IDs
    try:
        log.info(f"Loading manifest with pandas to read: '{grouping_col}'")
        df = pd.read_csv(manifest_path)
        if grouping_col not in df.columns:
            log.error(
                f"Grouping column '{grouping_col}' not found in {manifest_path}. "
                f"Available columns: {df.columns.tolist()}"
            )
            raise ValueError(f"Grouping column '{grouping_col}' not found.")
        
        # Get the list of strings
        all_grouping_strings = df[grouping_col].tolist()
        log.info(f"Successfully loaded {len(all_grouping_strings)} string IDs.")

    except FileNotFoundError:
        log.error(f"Manifest file not found at: {manifest_path}")
        return
    except Exception as e:
        log.error(f"Failed to load grouping strings from manifest: {e}")
        return

    # Now, configure the dataset to ONLY load features
    dataset_args = {
        "data_root": cfg.paths.data_root,
        "target_labels": label_cols,
        "visual_feature_col": visual_col,
        "text_feature_col": text_col,
        "preload": bool(OmegaConf.select(cfg, "data.preload_features", default=False)),
    }
    
    loader_args = {
        "batch_size": cfg.training.batch_size,
        "num_workers": cfg.training.num_workers,
        "pin_memory": True,
    }

    try:
        dataset = FeatureDataset(manifest_path=manifest_path, **dataset_args)
    except FileNotFoundError:
        log.error(f"Manifest file not found at: {manifest_path}")
        return
    except ValueError as e:
        log.error(f"Failed to initialize dataset: {e}")
        return
        
    dataloader = DataLoader(dataset, shuffle=False, **loader_args)

    # --- 4. Feature Extraction Loop ---
    all_visual_features = []
    all_text_features = []
    all_labels = []
    # Note: 'all_grouping_strings' was already loaded by pandas in Section 3

    log.info(f"Extracting features from {len(dataset)} samples...")
    try:
        with torch.no_grad():
            for batch in tqdm(dataloader, desc="Extracting features"):
                
                if "visual_features" not in batch:
                    raise KeyError(f"Batch missing 'visual_features'. Check config column: {visual_col}")
                if "text_features" not in batch:
                    raise KeyError(f"Batch missing 'text_features'. Check config column: {text_col}")
                
                all_visual_features.append(batch["visual_features"].to(device))
                all_text_features.append(batch["text_features"].to(device))

                if label_cols:
                    if "labels" not in batch:
                        raise KeyError("Batch missing 'labels'. Ensure label columns are configured correctly.")
                    all_labels.append(batch["labels"].to(device))

    except (FileNotFoundError, KeyError) as e:
        log.error(f"Failed during feature loading: {e}")
        log.error("Please ensure all feature files exist and manifest paths are correct.")
        return
    except TypeError as e:
        log.error(f"Failed processing batch. Error: {e}")
        return

    if not all_visual_features or not all_text_features:
        log.warning("No features were loaded. Cannot perform analysis.")
        return

    # Concatenate all batches into large tensors
    V = torch.cat(all_visual_features, dim=0)
    T = torch.cat(all_text_features, dim=0)
    
    log.info(f"Visual features tensor shape: {V.shape}")
    log.info(f"Text features tensor shape:   {T.shape}")
    log.info(f"Total grouping strings loaded: {len(all_grouping_strings)}")
    
    if V.shape[0] != T.shape[0] or V.shape[0] != len(all_grouping_strings):
        log.error("Mismatch in sample count between features and grouping strings.")
        return

    labels_tensor = None
    if label_cols:
        if not all_labels:
            log.error("Semantic metrics requested but no labels were loaded.")
            return

        labels_tensor = torch.cat(all_labels, dim=0)
        if labels_tensor.ndim == 1:
            labels_tensor = labels_tensor.unsqueeze(1)

        if labels_tensor.shape[0] != V.shape[0]:
            log.error("Mismatch in sample count between labels and features.")
            return

        log.info(f"Label tensor shape: {labels_tensor.shape}")
    
    # --- Filter normal cases if requested ---
    if filter_normal_cases:
        if labels_tensor is None:
            log.warning(
                "filter_normal_cases=True but no labels loaded. "
                "Cannot filter normal cases. Proceeding with all samples."
            )
        else:
            log.info("Filtering out normal cases (samples with all labels = 0)...")
            # Find samples where at least one label is non-zero
            has_pathology = labels_tensor.any(dim=1)
            num_pathology_cases = has_pathology.sum().item()
            num_normal_cases = (~has_pathology).sum().item()
            
            log.info(f"  Cases with pathology: {num_pathology_cases}")
            log.info(f"  Normal cases (filtered): {num_normal_cases}")
            
            if num_pathology_cases == 0:
                log.error("No cases with pathology found after filtering. Cannot proceed.")
                return
            
            # Filter all tensors and lists
            V = V[has_pathology]
            T = T[has_pathology]
            labels_tensor = labels_tensor[has_pathology]
            all_grouping_strings = [s for i, s in enumerate(all_grouping_strings) if has_pathology[i]]
            
            log.info(f"After filtering - Visual features shape: {V.shape}")
            log.info(f"After filtering - Text features shape: {T.shape}")
            log.info(f"After filtering - Grouping strings: {len(all_grouping_strings)}")

    # --- 5. Compute Similarity Matrix ---
    log.info("Calculating L2-normalized features...")
    V_norm = V / V.norm(dim=1, keepdim=True)
    T_norm = T / T.norm(dim=1, keepdim=True)

    log.info("Computing (N, N) similarity matrix...")
    similarity_matrix = V_norm @ T_norm.T
    log.info(f"Similarity matrix shape: {similarity_matrix.shape}")

    # --- 6. Generate Ground Truth Mask ---
    log.info("Parsing exam IDs from grouping strings (e.g., 'train_1_a_1' -> 'train_1_a')")
    try:
        # Se divide en el ÚLTIMO guion bajo y se toma la primera parte
        parsed_exam_ids = [s.rsplit('_', 1)[0] for s in all_grouping_strings]
        log.info(f"Successfully parsed {len(parsed_exam_ids)} IDs.")
        # log.debug(f"Example parsed IDs: {parsed_exam_ids[:5]}")
    except AttributeError as e:
        log.error(f"Failed to parse strings. Are 'labels' strings? Error: {e}")
        log.error(f"First 5 items in list: {all_grouping_strings[:5]}")
        raise

    log.info("Generating instance-level ground truth (GT) mask...")
    
    # Se usa NumPy para una comparación de strings 'broadcasted' eficiente
    parsed_ids_np = np.array(parsed_exam_ids)
    
    # [N, 1] == [1, N] -> [N, N]
    gt_mask_np = (parsed_ids_np[:, None] == parsed_ids_np[None, :])
    
    # Se convierte la máscara numpy a un tensor de PyTorch en el dispositivo
    gt_mask = torch.from_numpy(gt_mask_np).to(device)

    log.info(f"GT Mask shape: {gt_mask.shape}")
    log.info(f"Total correct pairs in GT Mask: {gt_mask.sum().item()}")

    semantic_mask = None
    if labels_tensor is not None:
        log.info("Generating semantic-level ground truth (GT) mask from label equality...")
        semantic_mask = torch.all(
            labels_tensor[:, None, :] == labels_tensor[None, :, :],
            dim=-1,
        )
        semantic_mask = semantic_mask.to(device)
        log.info(f"Semantic GT Mask shape: {semantic_mask.shape}")
        log.info(f"Total correct pairs in Semantic GT Mask: {semantic_mask.sum().item()}")

    # --- 7. Calculate Metrics ---
    log.info("Calculating alignment metrics...")
    metrics = {}
    metrics.update(
        calculate_metrics(
            similarity_matrix,
            gt_mask,
        )
    )

    if semantic_mask is not None:
        metrics.update(
            calculate_metrics(
                similarity_matrix,
                semantic_mask,
                prefix="semantic_",
                include_sample_count=False,
            )
        )

    # --- 8. Log Results ---
    log.info("="*70)
    log.info("ALIGNMENT ANALYSIS RESULTS")
    log.info("="*70)
    
    log.info("\n[INSTANCE-LEVEL METRICS]")
    log.info(f"  Samples Analyzed: {metrics.get('samples_analyzed', 0)}")
    log.info(f"  Mean Paired Similarity:   {metrics.get('mean_paired_similarity', float('nan')):.6f}")
    log.info(f"  Mean Unpaired Similarity: {metrics.get('mean_unpaired_similarity', float('nan')):.6f}")

    log.info("\n[VISUAL -> TEXT RETRIEVAL]")
    log.info(f"  MRR:        {metrics.get('v_to_t_mrr', 0.0):.4f}")
    log.info(f"  MAP:        {metrics.get('v_to_t_map', 0.0):.4f}")
    log.info("  Recall@K:")
    for k in TOP_K_VALUES:
        metric_value = metrics.get(f"v_to_t_recall_at_{k}", 0.0)
        log.info(f"    K={k:<2}:  {metric_value:.4f}")
    log.info("  NDCG@K:")
    for k in NDCG_K_VALUES:
        metric_value = metrics.get(f"v_to_t_ndcg_at_{k}", 0.0)
        log.info(f"    K={k:<2}:  {metric_value:.4f}")

    log.info("\n[TEXT -> VISUAL RETRIEVAL]")
    log.info(f"  MRR:        {metrics.get('t_to_v_mrr', 0.0):.4f}")
    log.info(f"  MAP:        {metrics.get('t_to_v_map', 0.0):.4f}")
    log.info("  Recall@K:")
    for k in TOP_K_VALUES:
        metric_value = metrics.get(f"t_to_v_recall_at_{k}", 0.0)
        log.info(f"    K={k:<2}:  {metric_value:.4f}")
    log.info("  NDCG@K:")
    for k in NDCG_K_VALUES:
        metric_value = metrics.get(f"t_to_v_ndcg_at_{k}", 0.0)
        log.info(f"    K={k:<2}:  {metric_value:.4f}")

    if semantic_mask is not None:
        log.info("\n" + "="*70)
        log.info("[SEMANTIC-LEVEL METRICS] (Exact Label Match)")
        log.info("="*70)
        log.info(f"  Mean Paired Similarity:   {metrics.get('semantic_mean_paired_similarity', float('nan')):.6f}")
        log.info(f"  Mean Unpaired Similarity: {metrics.get('semantic_mean_unpaired_similarity', float('nan')):.6f}")

        log.info("\n[SEMANTIC VISUAL -> TEXT RETRIEVAL]")
        log.info(f"  MRR:        {metrics.get('semantic_v_to_t_mrr', 0.0):.4f}")
        log.info(f"  MAP:        {metrics.get('semantic_v_to_t_map', 0.0):.4f}")
        log.info("  Recall@K:")
        for k in TOP_K_VALUES:
            metric_value = metrics.get(f"semantic_v_to_t_recall_at_{k}", float("nan"))
            log.info(f"    K={k:<2}:  {metric_value:.4f}")
        log.info("  NDCG@K:")
        for k in NDCG_K_VALUES:
            metric_value = metrics.get(f"semantic_v_to_t_ndcg_at_{k}", 0.0)
            log.info(f"    K={k:<2}:  {metric_value:.4f}")

        log.info("\n[SEMANTIC TEXT -> VISUAL RETRIEVAL]")
        log.info(f"  MRR:        {metrics.get('semantic_t_to_v_mrr', 0.0):.4f}")
        log.info(f"  MAP:        {metrics.get('semantic_t_to_v_map', 0.0):.4f}")
        log.info("  Recall@K:")
        for k in TOP_K_VALUES:
            metric_value = metrics.get(f"semantic_t_to_v_recall_at_{k}", float("nan"))
            log.info(f"    K={k:<2}:  {metric_value:.4f}")
        log.info("  NDCG@K:")
        for k in NDCG_K_VALUES:
            metric_value = metrics.get(f"semantic_t_to_v_ndcg_at_{k}", 0.0)
            log.info(f"    K={k:<2}:  {metric_value:.4f}")
        
    log.info("\n" + "="*70)


@hydra.main(version_base=None, config_path="../configs", config_name="config.yaml")
def main(cfg: DictConfig) -> None:
    """
    Hydra entry point for the alignment analysis script.
    """
    try:
        analyze_alignment(cfg)
    except Exception as e:
        log.exception(f"An error occurred during alignment analysis: {e}")
        raise


if __name__ == "__main__":
    main()