# CT-RATE Feature Benchmarks

This repository serves as the official benchmarking environment for developing and evaluating downstream models using pre-computed features from the **CT-RATE dataset**.

[cite_start]The goal of this project is to provide a standardized framework for testing various machine learning models on high-quality, pre-extracted visual and textual embeddings derived from the CT-RATE dataset's 3D chest CT scans and paired radiology reports[cite: 2, 4, 15].

## ğŸ¯ Project Objectives

This repository is *not* intended for 3D image processing or feature extraction. Instead, it focuses on the subsequent modeling tasks:

1.  **Visual-Only Classification:** Training and evaluating models (e.G., MLPs, linear probes) that use only the pre-computed 3D visual features as input.
2.  [cite_start]**Text-Only Classification:** Training and evaluating models that use only the pre-computed report embeddings (from findings and impressions sections)[cite: 18].
3.  [cite_start]**Multimodal Retrieval:** Benchmarking text-to-image and image-to-image retrieval tasks using the aligned latent spaces[cite: 23, 24, 25].
4.  **Configuration-Driven Experiments:** All experiments are managed via configuration files (Hydra) to ensure reproducibility.

## ğŸ—‚ï¸ Repository Structure

The repository follows a standard layout for reproducible machine learning projects:

```

ct-rate-feature-benchmarks/
â”œâ”€â”€ configs/               \# Hydra configuration files for experiments
â”œâ”€â”€ data/                  \# (Placeholder) Local storage for feature files/manifests
â”œâ”€â”€ notebooks/             \# Jupyter notebooks for exploration and analysis
â”œâ”€â”€ scripts/               \# Standalone scripts (e.g., data prep, batch runs)
â”œâ”€â”€ src/                   \# Main source code
â”‚   â”œâ”€â”€ ct\_rate\_benchmarks/  \# Core project package
â”‚   â”‚   â”œâ”€â”€ data/          \# Dataloaders and feature manifest handling
â”‚   â”‚   â”œâ”€â”€ models/        \# Model definitions (classifiers, retrieval)
â”‚   â”‚   â””â”€â”€ train.py       \# Main training script
â”œâ”€â”€ tests/                 \# Pytest unit and integration tests
â”œâ”€â”€ pyproject.toml         \# Project metadata and dependencies (for uv)
â””â”€â”€ README.md              \# This file

````

## ğŸš€ Getting Started

### 1. Environment Setup (using uv)

This project uses `uv` for package management.

```bash
# Create a virtual environment
python -m venv .venv
source .venv/bin/activate

# Install all dependencies (including dev and test)
uv pip install -e ".[dev,test]"
````

### 2\. (Next Steps)

  * Download pre-computed features (instructions to be added).
  * Run experiments using the training scripts (instructions to be added).

## ğŸ“š Background: CT-RATE and CT-CLIP

The features used in this repository originate from the CT-CLIP model, which was trained on the CT-RATE dataset.

  * [cite\_start]**CT-RATE Dataset:** A large-scale, multimodal dataset of **25,692 non-contrast 3D chest CT scans** paired with their corresponding radiology reports[cite: 3, 4]. [cite\_start]It includes 18 global abnormality labels for validation[cite: 10].
  * [cite\_start]**CT-CLIP Model:** A foundational contrastive learning model that aligns 3D CT volumes and text reports into a shared embedding space[cite: 15]. [cite\_start]This model is capable of powerful zero-shot classification and case retrieval[cite: 19, 23].

This repository utilizes the embeddings generated by CT-CLIP as the *input* for downstream tasks.